{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59fdfe6-81ca-441c-85a6-a49191a2d2fc",
   "metadata": {},
   "source": [
    "### Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a3d9e05-172f-4c83-9deb-a25a6a039113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['mstat', 'cendiv', 'gender', 'rahispan', 'raracem', 'raedyrs', 'ravetrn', 'shlt', 'shltc', 'depres', 'effort', 'sleepr', 'smokev', 'smoken', 'hibp', 'diab', 'cancr', 'lung', 'heart', 'strok', 'psych', 'arthr', 'slfmem', 'pstmem', 'spcfac', 'puffpos', 'covs', 'hiltc', 'lbrf']\n",
      "encodable categorical vars: ['mstat', 'cendiv', 'gender', 'rahispan', 'raracem', 'ravetrn', 'shlt', 'shltc', 'depres', 'effort', 'sleepr', 'smokev', 'smoken', 'hibp', 'diab', 'cancr', 'lung', 'heart', 'strok', 'psych', 'arthr', 'slfmem', 'pstmem', 'spcfac', 'puffpos', 'covs', 'hiltc', 'lbrf']\n",
      "Each hhidpn value appears only once.\n",
      "First half columns: Index(['raedyrs', 'mstat_5.divorced', 'effort_1.yes', 'logisret',\n",
      "       'strok_4.disp prev record and no cond', 'cendiv_6.es central',\n",
      "       'psych_4.disp prev record and no cond',\n",
      "       'raracem_2.black/african american', 'cancr_1.yes', 'hiltc_1.yes',\n",
      "       'loghatotb', 'shltc_-4', 'hiltc_0.no', 'shltc_-2', 'slfmem_3.good',\n",
      "       'hibp_1.yes', 'cesd', 'shlt_4.fair', 'shlt_5.poor', 'puffpos_2.sitting',\n",
      "       'psych_1.yes', 'mstat_2.married,spouse absent', 'lbrf_3.unemployed',\n",
      "       'mstat_7.widowed', 'cendiv_7.ws central', 'covs_0.no', 'raedyrs_17plus',\n",
      "       'ravetrn_1.yes', 'shltc_2', 'rahispan_1.hispanic', 'pstmem_3.worse',\n",
      "       'slfmem_4.fair', 'arthr_4.disp prev record and no cond',\n",
      "       'lbrf_4.partly retired', 'loghspti', 'shltc_4', 'hsptim', 'timwlk',\n",
      "       'spcfac_1.yes', 'sleepr_1.yes', 'cendiv_2.mid atlantic',\n",
      "       'mstat_4.separated', 'shltc_1', 'shlt_3.good'],\n",
      "      dtype='object')\n",
      "Second half columns: Index(['smoken_1.yes', 'gender_2.female', 'agey_m', 'lung_1.yes',\n",
      "       'lbrf_2.works pt', 'conde', 'cogtot', 'shltc_-3', 'isret',\n",
      "       'cendiv_4.wn central', 'lbrf_5.retired', 'cendiv_5.s atlantic',\n",
      "       'mstat_3.partnered', 'arthr_1.yes', 'cendiv_9.pacific',\n",
      "       'heart_4.disp prev record and no cond', 'cendiv_8.mountain', 'hatotb',\n",
      "       'cendiv_3.en central', 'smokev_1.yes',\n",
      "       'lung_4.disp prev record and no cond', 'wave', 'shltc_0', 'diab_1.yes',\n",
      "       'covs_1.yes', 'shlt_2.very good', 'logiearn', 'lbrf_6.disabled', 'bmi',\n",
      "       'pstmem_2.same', 'depres_1.yes', 'strok_1.yes', 'raracem_3.other',\n",
      "       'iearn', 'strok_2.tia/possible stroke', 'shltc_3', 'dage_m',\n",
      "       'heart_1.yes', 'puff', 'slfmem_5.poor', 'drinkn',\n",
      "       'mstat_8.never married', 'spcfac_0.no', 'slfmem_2.very good',\n",
      "       'lbrf_7.not in lbrf'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'Coding\\Git\\Economics_Research\\HRS Data Processing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18960\\1441856815.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Second half columns:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_second_half\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m \u001b[0mdf_first_half\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'first_half.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[0mdf_second_half\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'second_half.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3718\u001b[0m         )\n\u001b[0;32m   3719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3720\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         )\n\u001b[1;32m-> 1189\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;31m# Only for write methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[0mparent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'Coding\\Git\\Economics_Research\\HRS Data Processing'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hrs = pd.read_csv(\"life_expectancy_CleanedHRSdata.csv\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "### These are functions from the raw_data_processing.ipynb\n",
    "def standardize_dataframe(df, numerical_columns, exclude_columns):\n",
    "    \"\"\"\n",
    "    A function to standardize specified numerical values in a DataFrame using z-score normalization,\n",
    "    excluding specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas DataFrame): The input DataFrame.\n",
    "        numerical_columns (list): A list of numerical column names to standardize.\n",
    "        exclude_columns (list): A list of column names to exclude from standardization.\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame with standardized numerical values.\n",
    "    \"\"\"\n",
    "    # Exclude columns specified in exclude_columns\n",
    "    columns_to_standardize = [col for col in numerical_columns if col not in exclude_columns]\n",
    "    \n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the selected columns and transform the values\n",
    "    standardized_values = scaler.fit_transform(df[columns_to_standardize])\n",
    "    \n",
    "    # Create a new DataFrame with the standardized values and the same index and columns as the original DataFrame\n",
    "    standardized_df = pd.DataFrame(standardized_values, index=df.index, columns=columns_to_standardize)\n",
    "    \n",
    "    # Combine the standardized numerical columns with non-numerical columns from the original DataFrame\n",
    "    for col in df.columns:\n",
    "        if col not in columns_to_standardize:\n",
    "            standardized_df[col] = df[col]\n",
    "    \n",
    "    return standardized_df\n",
    "\n",
    "def encode_categorical(df, categorical_vars):\n",
    "    \"\"\"\n",
    "    A function to perform one-hot encoding for categorical variables in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas DataFrame): The input DataFrame.\n",
    "        categorical_columns (list): A list of column names containing categorical variables to be one-hot encoded.\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame with one-hot encoded categorical variables.\n",
    "    \"\"\"\n",
    "    print(f\"encodable categorical vars: {categorical_vars}\")\n",
    "    \n",
    "    # Convert numeric categorical variables to categorical type\n",
    "    for col in categorical_vars:\n",
    "        df[col] = df[col].astype('category')\n",
    "        \n",
    "    # Extract categorical variables\n",
    "    categorical_df = df[categorical_vars]\n",
    "    \n",
    "    # Perform one-hot encoding for categorical variables, drop first ensures there is no multicolinearity\n",
    "    result = pd.get_dummies(categorical_df, dtype=float, drop_first=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def replace_encoded_categorical(df, encoded_categorical_df, categorical_columns):\n",
    "    \"\"\"\n",
    "    A function to replace original categorical columns in a DataFrame with one-hot encoded columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas DataFrame): The original DataFrame.\n",
    "        encoded_categorical_df (pandas DataFrame): The DataFrame with one-hot encoded categorical variables.\n",
    "        categorical_columns (list): A list of column names containing original categorical variables.\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame with original categorical columns replaced by one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    # Drop original categorical columns from the original DataFrame\n",
    "    df = df.drop(columns=categorical_columns)\n",
    "    \n",
    "    # Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
    "    df = pd.concat([df, encoded_categorical_df], axis=1)\n",
    "    return df\n",
    "#columns_to_drop = ['Unnamed: 0', 'nt','n2','dage_y', 'rarelig']\n",
    "columns_to_drop = ['Unnamed: 0', 'nt','n2','dage_y', 'rarelig']\n",
    "hrs_cleaned = hrs.drop(columns=columns_to_drop, errors='ignore')\n",
    "    # Replace empty strings with NaN to handle both empty strings and NaN values uniformly\n",
    "hrs_cleaned = hrs_cleaned.replace('', np.nan)\n",
    "    \n",
    "    # Drop rows with any NaN values\n",
    "hrs_cleaned = hrs_cleaned.dropna()\n",
    "def random_sample_per_group(df, group_col):\n",
    "    \"\"\"\n",
    "    Randomly selects one observation per group from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to sample from.\n",
    "    group_col (str): The name of the column to group by.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with one randomly selected row per group.\n",
    "    \"\"\"\n",
    "    # Group by the specified column and apply the sampling\n",
    "    return df.groupby(group_col).apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "def is_categorical(df):\n",
    "    \"\"\"\n",
    "    Determines which columns in a DataFrame are categorical based on data type being a string.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas DataFrame): The DataFrame to check.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of column names that are considered categorical because they are of string type.\n",
    "    \"\"\"\n",
    "    categorical_vars = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    return categorical_vars\n",
    "\n",
    "# Determine categorical columns\n",
    "categorical_columns = is_categorical(hrs_cleaned)  # Adjust threshold as necessary\n",
    "print(\"Categorical columns:\", categorical_columns)\n",
    "# To remove columns\n",
    "categorical_columns = [col for col in categorical_columns if col not in ['raedyrs']]\n",
    "# Encode categorical variables\n",
    "encoded_df = encode_categorical(hrs_cleaned.copy(), categorical_columns)\n",
    "\n",
    "# Replace original categorical columns with encoded ones\n",
    "final_df = replace_encoded_categorical(hrs_cleaned, encoded_df, categorical_columns)\n",
    "\n",
    "# Convert all values to integers, setting '17.17+ yrs' specifically to 17\n",
    "final_df['raedyrs'] = final_df['raedyrs'].replace('17.17+ yrs', '17').astype(int)\n",
    "\n",
    "# Create a binary indicator for whether the education years are 17 and above\n",
    "final_df['raedyrs_17plus'] = (final_df['raedyrs'] >= 17).astype(int)\n",
    "\n",
    "# Check to make sure each \"hhidpn\" appears only once\n",
    "def random_sample_per_group(df, group_col):\n",
    "    \"\"\"\n",
    "    Randomly selects one observation per group from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to sample from.\n",
    "    group_col (str): The name of the column to group by.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with one randomly selected row per group.\n",
    "    \"\"\"\n",
    "    # Group by the specified column and apply the sampling\n",
    "    return df.groupby(group_col).apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "# Randomly sample one observation per individual\n",
    "final_df = random_sample_per_group(final_df, 'hhidpn')\n",
    "\n",
    "# Check to make sure each \"hhidpn\" appears only once\n",
    "def check_unique_hhidpn(sampled_df, group_col):\n",
    "    \"\"\"\n",
    "    Checks if each group identifier appears only once in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    sampled_df (pd.DataFrame): The DataFrame to check.\n",
    "    group_col (str): The name of the column to group by.\n",
    "    \"\"\"\n",
    "    if sampled_df[group_col].duplicated().any():\n",
    "        print(\"Some hhidpn values appear more than once.\")\n",
    "    else:\n",
    "        print(\"Each hhidpn value appears only once.\")\n",
    "\n",
    "# Perform the check\n",
    "check_unique_hhidpn(final_df, 'hhidpn')\n",
    "\n",
    "columns_to_drop = ['hhidpn', 'iwbeg', 'id']\n",
    "final_df = final_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "# Shuffle the columns randomly\n",
    "shuffled_columns = np.random.permutation(final_df.columns)\n",
    "\n",
    "# Split the columns approximately in half\n",
    "midpoint = len(shuffled_columns) // 2\n",
    "first_half_columns = shuffled_columns[:midpoint]\n",
    "second_half_columns = shuffled_columns[midpoint:]\n",
    "\n",
    "# Create two new DataFrames based on these split columns\n",
    "df_first_half = final_df[first_half_columns]\n",
    "df_second_half = final_df[second_half_columns]\n",
    "output_dir = 'Coding/Git/Economics_Research/HRS Data Processing'\n",
    "# Print out the columns to verify\n",
    "print(\"First half columns:\", df_first_half.columns)\n",
    "print(\"Second half columns:\", df_second_half.columns)\n",
    "\n",
    "df_first_half.to_csv(os.path.join(output_dir, 'first_half.csv'), index=False)\n",
    "df_second_half.to_csv(os.path.join(output_dir, 'second_half.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd58c467-353a-4645-8773-d56e504027e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['mstat', 'cendiv', 'gender', 'rahispan', 'raracem', 'raedyrs', 'ravetrn', 'shlt', 'shltc', 'depres', 'effort', 'sleepr', 'smokev', 'smoken', 'hibp', 'diab', 'cancr', 'lung', 'heart', 'strok', 'psych', 'arthr', 'slfmem', 'pstmem', 'spcfac', 'puffpos', 'covs', 'hiltc', 'lbrf']\n",
      "encodable categorical vars: ['mstat', 'cendiv', 'gender', 'rahispan', 'raracem', 'ravetrn', 'shlt', 'shltc', 'depres', 'effort', 'sleepr', 'smokev', 'smoken', 'hibp', 'diab', 'cancr', 'lung', 'heart', 'strok', 'psych', 'arthr', 'slfmem', 'pstmem', 'spcfac', 'puffpos', 'covs', 'hiltc', 'lbrf']\n",
      "Each hhidpn value appears only once.\n",
      "First half columns: Index(['heart_4.disp prev record and no cond', 'smoken_1.yes',\n",
      "       'arthr_4.disp prev record and no cond', 'mstat_4.separated',\n",
      "       'slfmem_3.good', 'lbrf_4.partly retired', 'shlt_4.fair',\n",
      "       'mstat_5.divorced', 'effort_1.yes', 'spcfac_0.no', 'raedyrs_17plus',\n",
      "       'shltc_-4', 'hiltc_0.no', 'raracem_2.black/african american',\n",
      "       'cancr_1.yes', 'lbrf_5.retired', 'logiearn', 'cendiv_9.pacific',\n",
      "       'slfmem_2.very good', 'lbrf_6.disabled', 'logisret', 'hsptim',\n",
      "       'hibp_1.yes', 'lung_4.disp prev record and no cond', 'spcfac_1.yes',\n",
      "       'lbrf_2.works pt', 'shlt_3.good', 'bmi', 'pstmem_3.worse', 'iearn',\n",
      "       'psych_1.yes', 'puffpos_2.sitting', 'gender_2.female', 'slfmem_4.fair',\n",
      "       'shltc_-3', 'cendiv_7.ws central',\n",
      "       'psych_4.disp prev record and no cond', 'loghspti',\n",
      "       'rahispan_1.hispanic', 'dage_m', 'shltc_1', 'loghatotb',\n",
      "       'mstat_3.partnered', 'hatotb'],\n",
      "      dtype='object')\n",
      "Second half columns: Index(['strok_4.disp prev record and no cond', 'covs_0.no', 'smokev_1.yes',\n",
      "       'lbrf_7.not in lbrf', 'cendiv_3.en central', 'cesd', 'puff',\n",
      "       'raracem_3.other', 'cendiv_6.es central', 'heart_1.yes', 'hiltc_1.yes',\n",
      "       'cendiv_4.wn central', 'timwlk', 'mstat_2.married,spouse absent',\n",
      "       'wave', 'agey_m', 'shltc_4', 'slfmem_5.poor', 'pstmem_2.same', 'isret',\n",
      "       'mstat_7.widowed', 'mstat_8.never married', 'drinkn',\n",
      "       'cendiv_5.s atlantic', 'shltc_3', 'cendiv_2.mid atlantic', 'shltc_2',\n",
      "       'shlt_2.very good', 'sleepr_1.yes', 'lung_1.yes', 'conde',\n",
      "       'lbrf_3.unemployed', 'cogtot', 'shlt_5.poor', 'raedyrs',\n",
      "       'cendiv_8.mountain', 'strok_1.yes', 'arthr_1.yes',\n",
      "       'strok_2.tia/possible stroke', 'diab_1.yes', 'ravetrn_1.yes',\n",
      "       'shltc_-2', 'shltc_0', 'depres_1.yes', 'covs_1.yes'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### These are functions from the raw_data_processing.ipynb\n",
    "def standardize_dataframe(df, numerical_columns, exclude_columns):\n",
    "    \"\"\"\n",
    "    A function to standardize specified numerical values in a DataFrame using z-score normalization,\n",
    "    excluding specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas DataFrame): The input DataFrame.\n",
    "        numerical_columns (list): A list of numerical column names to standardize.\n",
    "        exclude_columns (list): A list of column names to exclude from standardization.\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame with standardized numerical values.\n",
    "    \"\"\"\n",
    "    # Exclude columns specified in exclude_columns\n",
    "    columns_to_standardize = [col for col in numerical_columns if col not in exclude_columns]\n",
    "    \n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the selected columns and transform the values\n",
    "    standardized_values = scaler.fit_transform(df[columns_to_standardize])\n",
    "    \n",
    "    # Create a new DataFrame with the standardized values and the same index and columns as the original DataFrame\n",
    "    standardized_df = pd.DataFrame(standardized_values, index=df.index, columns=columns_to_standardize)\n",
    "    \n",
    "    # Combine the standardized numerical columns with non-numerical columns from the original DataFrame\n",
    "    for col in df.columns:\n",
    "        if col not in columns_to_standardize:\n",
    "            standardized_df[col] = df[col]\n",
    "    \n",
    "    return standardized_df\n",
    "\n",
    "def encode_categorical(df, categorical_vars):\n",
    "    \"\"\"\n",
    "    A function to perform one-hot encoding for categorical variables in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas DataFrame): The input DataFrame.\n",
    "        categorical_columns (list): A list of column names containing categorical variables to be one-hot encoded.\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame with one-hot encoded categorical variables.\n",
    "    \"\"\"\n",
    "    print(f\"encodable categorical vars: {categorical_vars}\")\n",
    "    \n",
    "    # Convert numeric categorical variables to categorical type\n",
    "    for col in categorical_vars:\n",
    "        df[col] = df[col].astype('category')\n",
    "        \n",
    "    # Extract categorical variables\n",
    "    categorical_df = df[categorical_vars]\n",
    "    \n",
    "    # Perform one-hot encoding for categorical variables, drop first ensures there is no multicolinearity\n",
    "    result = pd.get_dummies(categorical_df, dtype=float, drop_first=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def replace_encoded_categorical(df, encoded_categorical_df, categorical_columns):\n",
    "    \"\"\"\n",
    "    A function to replace original categorical columns in a DataFrame with one-hot encoded columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas DataFrame): The original DataFrame.\n",
    "        encoded_categorical_df (pandas DataFrame): The DataFrame with one-hot encoded categorical variables.\n",
    "        categorical_columns (list): A list of column names containing original categorical variables.\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame with original categorical columns replaced by one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    # Drop original categorical columns from the original DataFrame\n",
    "    df = df.drop(columns=categorical_columns)\n",
    "    \n",
    "    # Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
    "    df = pd.concat([df, encoded_categorical_df], axis=1)\n",
    "    return df\n",
    "#columns_to_drop = ['Unnamed: 0', 'nt','n2','dage_y', 'rarelig']\n",
    "columns_to_drop = ['Unnamed: 0', 'nt','n2','dage_y', 'rarelig']\n",
    "hrs_cleaned = hrs.drop(columns=columns_to_drop, errors='ignore')\n",
    "    # Replace empty strings with NaN to handle both empty strings and NaN values uniformly\n",
    "hrs_cleaned = hrs_cleaned.replace('', np.nan)\n",
    "    \n",
    "    # Drop rows with any NaN values\n",
    "hrs_cleaned = hrs_cleaned.dropna()\n",
    "def random_sample_per_group(df, group_col):\n",
    "    \"\"\"\n",
    "    Randomly selects one observation per group from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to sample from.\n",
    "    group_col (str): The name of the column to group by.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with one randomly selected row per group.\n",
    "    \"\"\"\n",
    "    # Group by the specified column and apply the sampling\n",
    "    return df.groupby(group_col).apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "def is_categorical(df):\n",
    "    \"\"\"\n",
    "    Determines which columns in a DataFrame are categorical based on data type being a string.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas DataFrame): The DataFrame to check.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of column names that are considered categorical because they are of string type.\n",
    "    \"\"\"\n",
    "    categorical_vars = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    return categorical_vars\n",
    "\n",
    "# Determine categorical columns\n",
    "categorical_columns = is_categorical(hrs_cleaned)  # Adjust threshold as necessary\n",
    "print(\"Categorical columns:\", categorical_columns)\n",
    "# To remove columns\n",
    "categorical_columns = [col for col in categorical_columns if col not in ['raedyrs']]\n",
    "# Encode categorical variables\n",
    "encoded_df = encode_categorical(hrs_cleaned.copy(), categorical_columns)\n",
    "\n",
    "# Replace original categorical columns with encoded ones\n",
    "final_df = replace_encoded_categorical(hrs_cleaned, encoded_df, categorical_columns)\n",
    "\n",
    "# Convert all values to integers, setting '17.17+ yrs' specifically to 17\n",
    "final_df['raedyrs'] = final_df['raedyrs'].replace('17.17+ yrs', '17').astype(int)\n",
    "\n",
    "# Create a binary indicator for whether the education years are 17 and above\n",
    "final_df['raedyrs_17plus'] = (final_df['raedyrs'] >= 17).astype(int)\n",
    "\n",
    "# Check to make sure each \"hhidpn\" appears only once\n",
    "def random_sample_per_group(df, group_col):\n",
    "    \"\"\"\n",
    "    Randomly selects one observation per group from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to sample from.\n",
    "    group_col (str): The name of the column to group by.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with one randomly selected row per group.\n",
    "    \"\"\"\n",
    "    # Group by the specified column and apply the sampling\n",
    "    return df.groupby(group_col).apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "# Randomly sample one observation per individual\n",
    "final_df = random_sample_per_group(final_df, 'hhidpn')\n",
    "\n",
    "# Check to make sure each \"hhidpn\" appears only once\n",
    "def check_unique_hhidpn(sampled_df, group_col):\n",
    "    \"\"\"\n",
    "    Checks if each group identifier appears only once in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    sampled_df (pd.DataFrame): The DataFrame to check.\n",
    "    group_col (str): The name of the column to group by.\n",
    "    \"\"\"\n",
    "    if sampled_df[group_col].duplicated().any():\n",
    "        print(\"Some hhidpn values appear more than once.\")\n",
    "    else:\n",
    "        print(\"Each hhidpn value appears only once.\")\n",
    "\n",
    "# Perform the check\n",
    "check_unique_hhidpn(final_df, 'hhidpn')\n",
    "\n",
    "columns_to_drop = ['hhidpn', 'iwbeg', 'id']\n",
    "final_df = final_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "# Shuffle the columns randomly\n",
    "shuffled_columns = np.random.permutation(final_df.columns)\n",
    "\n",
    "# Split the columns approximately in half\n",
    "midpoint = len(shuffled_columns) // 2\n",
    "first_half_columns = shuffled_columns[:midpoint]\n",
    "second_half_columns = shuffled_columns[midpoint:]\n",
    "\n",
    "# Create two new DataFrames based on these split columns\n",
    "df_first_half = final_df[first_half_columns]\n",
    "df_second_half = final_df[second_half_columns]\n",
    "\n",
    "# Print out the columns to verify\n",
    "print(\"First half columns:\", df_first_half.columns)\n",
    "print(\"Second half columns:\", df_second_half.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a87596-8e97-423f-a8af-d5d7c1c76cee",
   "metadata": {},
   "source": [
    "### VAE Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac3f53a-7363-4a47-8b63-0d2ebb30767a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Vectorize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18960\\1315431353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojects_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mVectorize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPreprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mraw_dataframe_preprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPredict\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpredictors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moracle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Vectorize'"
     ]
    }
   ],
   "source": [
    "from multiprocessing import freeze_support\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "projects_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(projects_directory)\n",
    "\n",
    "from Vectorize import VAE\n",
    "from Preprocess import raw_dataframe_preprocessor, column_optimizer\n",
    "from Predict import predictors as oracle\n",
    "from Vectorize import Encoder\n",
    "from HNSW import Row_Matcher\n",
    "\n",
    "def main():\n",
    "    # Assume df_first_half and df_second_half are already in memory\n",
    "    first_half_preprocessed = df_first_half\n",
    "    first_half_X = first_half_preprocessed.drop('target_variable', axis=1)  # Replace 'target_variable' with the actual target variable name\n",
    "    \n",
    "    print(\"First half dataset loaded\")\n",
    "    \n",
    "    second_half_preprocessed_whole = df_second_half\n",
    "    second_half_X_whole = second_half_preprocessed_whole.drop('target_variable', axis=1)  # Replace 'target_variable' with the actual target variable name\n",
    "    second_half_y_whole = second_half_preprocessed_whole['target_variable']  # Replace 'target_variable' with the actual target variable name\n",
    "    second_half_X_whole_tensor = torch.tensor(second_half_X_whole.values, dtype=torch.float)\n",
    "    second_half_y_whole_tensor = torch.tensor(second_half_y_whole.values, dtype=torch.float)\n",
    "    \n",
    "    print(\"Second half dataset loaded\")\n",
    "    \n",
    "    second_half_X_whole_with_target = oracle.run_model_pipeline_and_return_final_heart_predictors(None, None, second_half_X_whole)\n",
    "    \n",
    "    print(\"Model pipeline run on second half dataset\")\n",
    "    \n",
    "    ### Column rearranging \n",
    "    column_rearranger = column_optimizer.ColumnRearranger()\n",
    "    \n",
    "    # Reduce row number of second half table to match that of first half via bootstrapping\n",
    "    second_half_X = column_rearranger.bootstrap_to_match(first_half_X, second_half_X_whole_with_target)\n",
    "    \n",
    "    print(\"Second half dataset bootstrapped to match first half\")\n",
    "    \n",
    "    raw_dataframe_preprocessor.save_dataframe(second_half_X, current_directory+\"/PVM/Datasets\", \"second_half_preprocessed_X.csv\")\n",
    "    \n",
    "    # pre-rearrangement\n",
    "    average_correlation_pre = column_rearranger.compute_average_correlation(first_half_X, second_half_X)\n",
    "    print(f\"Pre-operation average correlation: {average_correlation_pre}\")\n",
    "    \n",
    "    # Rearrange columns of the right table such that the average correlation between every column i from the left table and every column j from the right table where i=j is maximized\n",
    "    second_half_X_rearranged = column_rearranger.return_optimal_rearrangement(first_half_X, second_half_X)\n",
    "    \n",
    "    print(\"Second half dataset columns rearranged\")\n",
    "    \n",
    "    # post-rearrangement\n",
    "    average_correlation_post = column_rearranger.compute_average_correlation(first_half_X, second_half_X_rearranged)\n",
    "    print(f\"Post-operation average correlation: {average_correlation_post}\")\n",
    "    \n",
    "    column_rearranger.visualize_comparison(average_correlation_pre, average_correlation_post)\n",
    "    \n",
    "    # Update global data\n",
    "    raw_dataframe_preprocessor.update_heart_final_predictors(second_half_X_rearranged, list(second_half_X_rearranged.columns))\n",
    "    \n",
    "    print(\"Global data updated with rearranged second half dataset\")\n",
    "    \n",
    "    # Visualize if rearrangement was done correctly\n",
    "    raw_dataframe_preprocessor.save_dataframe(second_half_X_rearranged, current_directory+\"/PVM/Datasets\", \"second_half_preprocessed_X_rearranged.csv\")\n",
    "    \n",
    "    ### Adding the Vector Encoded Column that summarizes each row's data using VAE\n",
    "    encoder = Encoder.DataFrameEncoder()\n",
    "    # Train the models\n",
    "    encoder.train_and_assign_models(first_half_preprocessed, second_half_X_rearranged, second_half_preprocessed_whole)\n",
    "    # Save the models\n",
    "    encoder.save_model(encoder.first_half_model, 'first_half_model.pth')\n",
    "    encoder.save_model(encoder.second_half_model, 'second_half_model.pth')\n",
    "    # Add a vector encoding column to first half and second half dataframes\n",
    "    encoded_first_half_df, encoded_second_half_df = encoder.load_and_encode_dataframes(first_half_X, second_half_X_rearranged)\n",
    "    \n",
    "    print(\"Vector encoding added to first half and second half datasets\")\n",
    "    \n",
    "    raw_dataframe_preprocessor.save_dataframe(encoded_first_half_df, current_directory+\"/PVM/Datasets\", \"first_half_predictors.csv\")\n",
    "    print(f\"First half predictors:\\n{encoded_first_half_df.head()}\")\n",
    "    \n",
    "    raw_dataframe_preprocessor.save_dataframe(encoded_second_half_df, current_directory+\"/PVM/Datasets\", \"second_half_predictors.csv\")\n",
    "    print(f\"Second half predictors:\\n{encoded_second_half_df.head()}\")\n",
    "    \n",
    "    ### Probabilistic Vectorized Matching\n",
    "    \n",
    "    # HYPERPARAMETERS\n",
    "    batch_size, num_training_updates, num_hiddens, embedding_dim, learning_rate = VAE.return_hyperparameters()\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    first_half_model = VAE.Model(encoded_first_half_df.shape[1], num_hiddens, embedding_dim).to(device)\n",
    "    first_half_model.load_state_dict(torch.load('first_half_model.pth'))\n",
    "    first_half_model.eval()\n",
    "    \n",
    "    second_half_model = VAE.Model(encoded_second_half_df.shape[1], num_hiddens, embedding_dim).to(device)\n",
    "    second_half_model.load_state_dict(torch.load('second_half_model.pth'))\n",
    "    second_half_model.eval()\n",
    "    \n",
    "    print(\"VAE models loaded\")\n",
    "\n",
    "    def match_rows(first_half_df, second_half_df):\n",
    "        row_matcher = Row_Matcher.RowMatcher()\n",
    "        return row_matcher.retrieve_similar(first_half_df, second_half_df)\n",
    "    \n",
    "    # Perform row matching and store results\n",
    "    combined_data = match_rows(encoded_first_half_df, encoded_second_half_df)\n",
    "    \n",
    "    print(\"Row matching performed\")\n",
    "    \n",
    "    ### Prepare Data for Final OLS Regression\n",
    "    final_first_half_regressors, final_second_half_regressors, final_first_half_y, final_second_half_y = raw_dataframe_preprocessor.return_final_variables()\n",
    "    combined_data[final_first_half_y] = first_half_preprocessed[final_first_half_y]\n",
    "    combined_data[final_second_half_y] = second_half_preprocessed_whole[final_second_half_y]\n",
    "    \n",
    "    print(\"Data prepared for final OLS regression\")\n",
    "\n",
    "    # Save and display results\n",
    "    combined_data.to_csv(current_directory + \"/PVM/Datasets/merged_predictors.csv\")\n",
    "    print(f\"Merged predictors:\\n{combined_data.head()}\")\n",
    "    \n",
    "    \"\"\"\n",
    "        FINAL RESULTS\n",
    "    \"\"\"\n",
    "    oracle.test_research_null(combined_data, combined_data[final_first_half_regressors], combined_data[final_second_half_regressors], combined_data[final_first_half_y], combined_data[final_second_half_y])\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ### Multiprocessing for deep learning\n",
    "    freeze_support()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "552df862-4ca5-46ab-81b6-a91b16b9ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(current_directory, 'modules'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a9e45-0c3d-47d1-8912-f00b19ce9c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
